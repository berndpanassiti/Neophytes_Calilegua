---
title: "Boosted regression trees"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
source("r-code/00_functions.R")
load("data/workingdata.RData")
using("ggplot2","tidyverse","purrr","dplyr","broom","knitr","dismo","skimr","caret")

rm(.Random.seed, envir=globalenv())
set.seed(2020)
```

https://rspatial.org/raster/sdm/9_sdm_brt.html
https://statistik-dresden.de/archives/14967
http://uc-r.github.io/gbm_regression

Input data:
env = environmental variables
plants = selected neophytes + species richness


```{r}
dat = preds %>% dplyr::select(ring) # colSums(is.na(df))

# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- caret::dummyVars( ~ ., data=dat)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
GBMpreds <- data.frame(predict(dummies_model, newdata = dat))
#GbmPlantsPreds = data.frame(plants,GBMpreds[,-5],slope=preds$slope,treeAbund.4 = preds$treeAbund.4,plant_comunity.altered.forest=preds$plant_comunity.altered.forest)

#GbmPlantsPreds = data.frame(plants,GBMpreds[,-5],activity=preds$activity,slope=preds$slope,treeAbund.4 = preds$treeAbund.4)
# plant1 ony 1 model that does not converge

#GbmPlantsPreds = data.frame(plants,GBMpreds[,-5],activity=preds$activity,slope=preds$slope,plant_comunity.altered.forest=preds$plant_comunity.altered.forest)
# no models for plant2

#GbmPlantsPreds = data.frame(plants,GBMpreds[,-5],activity=preds$activity,treeAbund.4 = preds$treeAbund.4,plant_comunity.altered.forest=preds$plant_comunity.altered.forest)
# plant1 only 1 model and that model does not converge

#GbmPlantsPreds = data.frame(plants,GBMpreds[,-5],activity=preds$activity,plant_comunity.forest=preds$plant_comunity.forest,plant_comunity.altered.forest=preds$plant_comunity.altered.forest)
# plant 1 does not converge

GbmPlantsPreds = data.frame(plants,GBMpreds[,-5],activity=preds$activity,soil_type.2=preds$soil_type.2,plant_comunity.altered.forest=preds$plant_comunity.altered.forest)
# plants 2 does not converge!

#GbmPlantsPreds = data.frame(plants,GBMpreds[,-5],activity=preds$activity,herbAbund.4=preds$herbAbund.4,plant_comunity.shrubs=preds$plant_comunity.shrubs)
# plants 1 and 2 with at least 2 warnings  glm.fit: fitted probabilities numerically 0 or 1 occurred and sometimes, glm.fit: algorithm did not converge

#GbmPlantsPreds = data.frame(plants,GBMpreds[,-5],activity=preds$activity,herbAbund.4=preds$herbAbund.4,plant_comunity.shrubs=preds$plant_comunity.shrubs)
# plants 1 and 2 with at least 2 warnings  glm.fit: fitted probabilities numerically 0 or 1 occurred and sometimes, glm.fit: algorithm did not converge


#GbmPlantsPreds = data.frame(plants,GBMpreds[,-5],activity=preds$activity,plant_comunity.altered.forest=preds$plant_comunity.altered.forest)
# no convergence for both plants


#GbmPlantsPreds = data.frame(plants,GBMpreds[,-5],activity=preds$activity,soil_type.4=preds$soil_type.4,plant_comunity.altered.forest=preds$plant_comunity.altered.forest)
# plant2 with no models
```






# Tuning of parameters
bag.fraction: the fraction of the training set observations randomly selected to propose the next tree in the expansion
shrinkage/learning.rate: Die „Lerngeschwindigkeit“ – betrifft die Gewichtung schlecht vorhergesagter Fälle für den nachfolgenden Baum. 
interaction.depth/tree.complextity: Die „Tiefe“ der Bäume, d. h. die Anzahl der Ebenen. (Der Beispiel-Baum ganz oben enthält zwei Verzweigungsebenen.) 


```{r}
# create hyperparameter grid
hyper_grid <- expand.grid(
  learning.rate = c(0.0001,0.0005,0.001),
  bag.fraction = c(.5,.65,.75), 
  tree.complexity  = c(1,3,5),
  optimal_trees = 0,                # a place to dump results
  min_RMSE = 0,                     # a place to dump results
  cv.deviance.mean = 0,             # a place to dump results
  cv.deviance.se = 0,               # a place to dump results
  cv.correlation.mean = 0,          # a place to dump results
  cv.correlation.se = 0            # a place to dump results
)
```



# Creation of gbms based on hypergrid for all seleceted neophytes and species richness
Species richness based on 6 neophytes


```{r}
df=GbmPlantsPreds

gbm.tune.result=list()
bgm.family ="bernoulli" # for individual neophytes

for(s in 3:6) {
 if(s %in% c(5,6)) {bgm.family = "poisson"}

#for(s in 1:2) {
 

for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(2020)
  
    # train model
  gbm.tune <- gbm.step(
    data=df, gbm.x = 7:ncol(df), gbm.y = s,
    family = bgm.family,
    learning.rate = hyper_grid$learning.rate[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    tree.complexity = hyper_grid$tree.complexity[i],
    n.folds=10,
    verbose=F
  )
  
  hyper_grid$optimal_trees[i]       <- ifelse(is.null(gbm.tune$n.trees),NA , gbm.tune$n.trees) # which.min(gbm.tune$train.error) # =
  hyper_grid$min_RMSE[i]            <- ifelse(is.null(sqrt(min(gbm.tune$train.error))),NA ,sqrt(min(gbm.tune$train.error)))
  hyper_grid$cv.deviance.mean[i]    <- ifelse(is.null(gbm.tune$cv.statistics$deviance.mean),NA ,gbm.tune$cv.statistics$deviance.mean)
  hyper_grid$cv.deviance.se[i]      <- ifelse(is.null(gbm.tune$cv.statistics$deviance.se),NA ,gbm.tune$cv.statistics$deviance.se)
  hyper_grid$cv.correlation.mean[i] <- ifelse(is.null(gbm.tune$cv.statistics$correlation.mean),NA ,gbm.tune$cv.statistics$correlation.mean)
  hyper_grid$cv.correlation.se[i]   <- ifelse(is.null(gbm.tune$cv.statistics$correlation.se),NA ,gbm.tune$cv.statistics$correlation.se)
  hyper_grid$AUCtrain.mean[i]       <- ifelse(is.null(gbm.tune$cv.statistics$discrimination.mean),NA , gbm.tune$cv.statistics$discrimination.mean)
  hyper_grid$AUCtrain.se[i]         <- ifelse(is.null(gbm.tune$cv.statistics$discrimination.se),NA , gbm.tune$cv.statistics$discrimination.se)

}
print(hyper_grid)

#gbm.result = hyper_grid %>% arrange(-desc(min_RMSE)) %>% dplyr::filter(optimal_trees > 999)
gbm.result= hyper_grid %>% arrange(-desc(min_RMSE)) 
gbm.tune.result[[s]] = data.frame(gbm.result)
}


gbm.tune.result.df=as_tibble(data.frame(cbind(y=rep(1:6,each=nrow(gbm.tune.result[[1]])),plyr::rbind.fill(gbm.tune.result))))
gbm.tune.result.tree1000 = gbm.tune.result.df %>% dplyr::filter(optimal_trees > 999)
```





```{r}
df=GbmPlantsPreds

# plant1
s=1

set.seed(2020)     # reproducibility
    # train model
  gbm.tune <- gbm.step(
    data=df, gbm.x = 7:ncol(df), gbm.y = s,
    family = bgm.family,
    learning.rate = 0.001,
    bag.fraction = 0.7,
    tree.complexity = 5,
    n.folds=10,
    verbose=F
)
# soil.2 ok
  
 
# plant1
s=2

set.seed(2020)     # reproducibility
    # train model
  gbm.tune <- gbm.step(
    data=df, gbm.x = 7:ncol(df), gbm.y = s,
    family = bgm.family,
    learning.rate = 0.00001,
    bag.fraction = 1,
    tree.complexity = 5,
    n.folds=10,
    verbose=F
)
# soil.2 with 2 wwarings glm.fit: fitted probabilities numerically 0 or 1 occurred
```







## Recheck parameter values for models with warnings

Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 


```{r}
df=GbmPlantsPreds
bgm.family ="bernoulli" # for individual neophytes

gbm.tune.result.1 = as_tibble(data.frame(matrix(0,ncol=12,nrow=7)))
colnames(gbm.tune.result.1) = colnames(gbm.tune.result.tree1000)

fittedValues = as_tibble(data.frame(matrix(0,ncol=4,nrow=143)))

# plant1
s=1
tuneParameters=gbm.tune.result.tree1000[gbm.tune.result.tree1000$y==s,]

tuneParameter=tuneParameters[28,] # test individual settings!
set.seed(2020)     # reproducibility
    # train model
  gbm.tune <- gbm.step(
    data=df, gbm.x = 7:ncol(df), gbm.y = s,
    family = bgm.family,
    learning.rate = tuneParameter$learning.rate,
    bag.fraction = tuneParameter$bag.fraction,
    tree.complexity = tuneParameter$tree.complexity,
    n.folds=10,
    verbose=F
)
# ok!
gbm.tune.result.1[s,] = tuneParameter
fittedValues[,s] = gbm.tune$fitted
  

# plant2
s=2
tuneParameters=gbm.tune.result.tree1000[gbm.tune.result.tree1000$y==s,]

tuneParameter=tuneParameters[15,] # test individual settings!
set.seed(2020)     # reproducibility
    # train model
  gbm.tune <- gbm.step(
    data=df, gbm.x = 7:ncol(df), gbm.y = s,
    family = bgm.family,
    learning.rate = 0.00001,
    bag.fraction = 1,
    tree.complexity = 5,
    n.folds=10,
    verbose=F
)
# # A tibble: 1 x 12
#       y learning.rate bag.fraction tree.complexity optimal_trees min_RMSE cv.deviance.mean cv.deviance.se cv.correlation.mean cv.correlation.se AUCtrain.mean AUCtrain.se
#   <int>         <dbl>        <dbl>           <dbl>         <dbl>    <dbl>            <dbl>          <dbl>               <dbl>             <dbl>         <dbl>       <dbl>
# 1     2       0.00001          1               5          1000    0.800            0.638         0.0448               0.115             0.100         0.635      0.0857

# ok!
gbm.tune.result.1[s,] = tuneParameter
fittedValues[,s] = gbm.tune$fitted
   
```



































# plots
```{r}

gbm.plot.fits(final.gbm.MEM)



df2 <- summary(final.gbm.MEM) %>% 
  dplyr::rename("variable" = var) %>% 
  dplyr::arrange(rel.inf) %>%
  dplyr::mutate(variable = forcats::fct_inorder(variable))

ggplot2::ggplot(df2) +
  geom_col(aes(x = variable, y = rel.inf),
           col = "black", show.legend = F) +
  coord_flip() +
  scale_fill_grey() +
  theme_bw()

# library(vip)
# devtools::install_github("koalaverse/vip")
vip::vip(gbm.fit.final)
```














































